# 🕷️DebugEval 
Official repository for the paper "Enhancing the Code Debugging Ability of LLMs via
Communicative Agent Based Data Refinement".

<p align="center">
    <a href="">📜 Paper</a> •
    <a href="">🤗 Data </a> •
    <a href="">🤖 Model </a> •
    <a href="">🏆 Leaderboard</a> 
</p>

## 1. Introduction
This paper presents a benchmark, DebugEval, which is used to evaluate the code debugging ability of LLMs (Large Language Models) and proposals a framework for building training data using multiple agents, MASTER.

### 1.1 Benchmark
DebugEval designs four task scenarios: BUG Localization, BUG Identification, Code Repair, and Code Review to comprehensively evaluate the code debugging capability of LLMs.
![image](Figure/benchmarck_00.png)
### 1.2 MASTER
MASTER is a framework for making use of multiple agents working together to refine training data to improve code debugging capability in LLMs.

## 2. Installation
You can clone the repository using the following command:

```
git clone DebugEval
cd DebugEval
```

## 3. Inference and Evaluation

## 4. Fine-Tuning

## 5. Citation
